\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage[utf8]{inputenc}

\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{listings}  
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{.2,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0.4,0.82}
\definecolor{codeorange}{rgb}{0.94,0.34,0.0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backcolourgray}{rgb}{0.92,0.92,0.92}
\definecolor{codewhite}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolourgray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    extendedchars=true,
    frame=single
    %, basicstyle=\footnotesize
}
\lstset{style=mystyle}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{Arquitectura de ordenadores. Practice 4.}
\lhead{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
\cfoot{\thepage}



%\setlength{\parskip}{0.15cm}


%parameters: file, caption, label, scale
\newcommand{\myFigure}[4]{%
    \begin{figure}[!ht]
        \includegraphics[width=#4\textwidth]{#1}
        \centering
        \caption{#2}
        \label{#3}
    \end{figure}
}
%grey item for enumerate
\newcommand{\greyItem}[1]{\item\emph{\textcolor{darkgray}{#1}}}


\begin{document}


\title{\textbf{Arquitectura de ordenadores. Practice 4.}}
\author{\textbf{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}\\(group 1392, team 04)}
%\date{}
\maketitle

\begin{tcolorbox}
\tableofcontents
\end{tcolorbox}


\newpage
\section{Exercise 1: Basic OpenMP programs}

\begin{enumerate}[label=1.\arabic*,leftmargin=*]
\greyItem{Is it possible run more threads than cores on the system? Does it make sense to do it?}

It is possible to do so: if we first execute omp1 without arguments to see the available cores and then we execute it again with a number of threads higher than the amount of cores, we will see that the execution is carried out without problems. Using more threads than cores may make no sense when the goal is simply to parrallelize a single task because the threads will interrupt each other. However, if there are several tasks to be run at the same time and any of them may provoke the thread to go asleep, then using more threads could help to take advantage of the time that particular thread is asleep. 

%Quitar ultimo párrafo??

\greyItem{How many threads should you use on the computers in the lab? And in the cluster? And on your own team?} 

For a general situation, the number of threads to be used is the one determined by the function \emph{omp\_get\_max\_threads()}. Indeed this value used by default.

\greyItem{Modify the omp1.c program to use the three ways to choose the number of threads and try to guess the priority among them}

The priority order is: \texttt{}{OMP\_NUM\_THREADS} $<$ \texttt{omp\_set\_num\_threads()} $<$ \#pragma\ omp parallel num\_threads(numthr) 

\greyItem{How does OpenMP behave when we declare a private variable?}

Each thread created will create its own (new) variable.

\greyItem{What happens to the value of a private variable when the parallel region starts executing?}

The variable is not necessarily initialized when allocated in each thread stack, although when executing \texttt{omp2} in the lab computers, it is initialized as if it were a \emph{firstprivate} variable (with the value of the variable previous to the parallel section). Furthermore, it is private to each thread so its value will only change because of instructions performed by that specific thread.

\greyItem{What happens to the value of a private variable at the end of the parallel region?}

After the parallel region, the master threads resumes execution and the value of the variable is the one from before the parallel execution. Privates variable are local to the threads and the parallel region, so they are deleted when the threads terminate.

\greyItem{Does the same happen with public variables?}

Public variables are shared by all threads (no copies of the variable are created for each thread); threfore, after the parallel region, the value will be determined by the code executed in the parallel region (and, perhaps, by the order of execution of the several threads).

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2: Parallelize the dot product}

\begin{enumerate}[label=2.\arabic*,leftmargin=*]

\greyItem{Run the serial version and understand what the result should be for different vector sizes.} 
    
As we are calculating the dot product of two vectors of size M where all the components are 1, the expected output is M.
    
\greyItem{Run the parallelized code with the openmp pragma and answer the following questions in the document}
    
The parallel version is not working due to the fact that variable sum is being shared (default mode). So we have severals threads writing in the same variable at the same time which produces a data race.

\greyItem{Modify the code and name the program pescalar\_par2. This version should give the correct result using the appropriate pragma:}

The modification needed is adding the pragma clause just before the calculation.
Both \emph{pragmas} solve the data race. Nonetheless, the way they achieved so is different. \emph{Critial} pragma implies the use of a mutex so only one thread can access the block at a time (which is quite expensive in execution time) whereas \emph{atomic} ensures that the whole calculation (line 42) is executed as a single operation. The result is exactly the same as we are enclosing just a sentence of code, however, the second option is considerably faster.

\begin{lstlisting}[language=C, texcl=true]
    ...
    #pragma omp parallel for 
    for(k=0;k<M;k++)
    {	
        #pragma omp atomic
            sum = sum + A[k]*B[k];
    } 
    ...
\end{lstlisting}

\greyItem{Modify the code and name the resulting program pescalar\_par3. This version should give the correct result using the appropriate pragma: }

If we use \emph{reduction} we get better results than the ones obtained with \emph{atomic}. This is the appropriate solution in this case, because it is the optimized version meant for this specific task.

\greyItem{Run time analysis}

We have written a script to test this (\emph{scr2\_threshold.sh}). As can be seen in the following figure (fig. \ref{threshold}), the parallel version (in the lab computers, that use 4 threads) is very inconsistent. However, the first value that meets the requirements is $T=1200000$:

\myFigure{../material/outputs/out2_final/threshold0.png}{Search for threshold in the lab computers}{threshold}{.7}

\pagebreak

\greyItem{(Optional) Exhaustive analysis}

Executing the same tests in the cluster, using also 4 cores, we have reached the value that meets the requirements at, aproximately, $T=81000$, as can be seen in the following figure (\ref{threshold_cluster4}):

\myFigure{../material/outputs/out2_final/threshold_4proc.png}{Search for threshold in the cluster: 4 cores}{threshold_cluster4}{.66}

Changing the number of threads to 8, the number increases slightly, the threshold would be around $T=90000$. This makes sense, because, although using 8 threads would yield a much better performance for very large matrix sizes, the threshold for which creating a higher number of threads is worth the time spent creating them is also a bit higher.

\myFigure{../material/outputs/out2_final/threshold_8proc.png}{Search for threshold in the cluster: 8 cores}{threshold_cluster8}{.66}

\end{enumerate}


\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 3: Parallel matrix multiplication}

We have sampled times for the different programs using matrices of size 2000, which yield high enough times to make a difference between the different implementations. Here are the results:

\begin{figure}[h]
    \input{./tables/table3_n2000_1}
    \centering
    \caption{Times (seconds) for the requested tests}
\end{figure}

\begin{figure}[h]
    \input{./tables/table3_n2000_2}
    \centering
    \caption{Ratios for the requested tests}
\end{figure}

\begin{enumerate}[label=3.\arabic*,leftmargin=*]
\greyItem{ Which of the three versions performs the worst? Why? Which of the three versions performs better? Why?}

The worst performing version is the one that parallelizes the innermost loop, because it is forced to ``fork'' itself $N^2$ times, which causes the execution to be a lot slower. The best performing one, as could be expected, is the one that parallelizes the outermost loop, it has to separate into different threads only once (the \emph{multiplication\_loop2.c} one ``forks'' $N$ times).

\greyItem{Based on the results, do you think fine-grained (innermost loop) or coarse-grained (outermost loop) parallelization is preferable in other algorithms?}

Based on these results, coarse-grained parallelization is generally preferable, as it avoids slowing the execution by killing and creating threads multiple times.

\end{enumerate}

Taking the best parallel version (parallelizing the outermost loop) alongside the serial version, we have tested for $N$ between $512+P$ and $512+1024+P$ with increments of $64$. And with the parallel version, we have tested for $2$, $4$ and $6$ threads. These are the results: 

\pagebreak

\myFigure{../material/outputs/out3/test_cl0/t_fig.png}{Time in seconds.}{times_e3}{0.65}

As expected, the plots have a cubic behaviour. Comparing the different plots, as the values of $N$ are sufficiently large to take advantage of the parallelization, the higher the number of threads, the less time it takes to compute the multiplication. The serial version is the slowest for all of these values of $N$.

\myFigure{../material/outputs/out3/test_cl0/r_fig.png}{Time ratios.}{ratios}{0.65}

As commented before, for the sizes that we have tested, the asymptotic behaviour is already present (as $N$ is big enough). The four time fuctions have a complexity of $\mathcal{O}(N^3)$, which means that if we calculate the speedup for each parallel version (with different number of threads), it will stabilize around a constant in each case. This constant behaviour of the ratios is what can be seen in this plot. Again, as lower number of threads is worse than higher number of threads for big enough values of $N$, the constant in question is higher when the number of threads is higher.

\begin{enumerate}[label=3.\arabic*,leftmargin=*]
    \setcounter{enumi}{2}
\greyItem{If in the previous chart you did not obtain a behavior of the acceleration as a function of N that stabilizes or decreases with increasing the size of the matrix, continue increasing the value of N until you get a chart with this behavior and indicate for which value of N you begin to see the change in trend.}

The values that we got already stabilize the speedup; as can be seen in figure \ref{ratios}, the values stabilize: for 2 threads, around $2.6$; for 4 threads, around $3.1$; and 6 threads, $4.6$.

In order to check the values of $N$ that stabilize the speedup, we have to check lower values:

\myFigure{../material/outputs/out3/test_cl0_extended/r_fig.png}{Time ratios extending the plot with lower values.}{ratios_extended}{0.63}

As we can see, with increasing number of threads, the sabilizing value increases slightly. For 2 threads, the value is around $N=100$; for 4 threads, between $N=130$ and $N=400$, and for 6 threads, between $N=160$ and $N=450$.


\end{enumerate}

\pagebreak

\section{Exercise 4: Example of numerical integration}

\begin{enumerate}[label=4.\arabic*,leftmargin=*]

\greyItem{How many rectangles are used in the program? Which value does h take?}

The numer of rectangles is determined by the variable \emph{n}, whose value is ${10^8}$. The variable \emph{h} takes the length of the segments in which the interval $[0,1]$ is divided (the base of the rectangles), so it is $h=1/n$. In this case, $h=10^{-8}$.

\greyItem{ Run all versions of the program. Analyze the performance (mean execution time and speedup) and assessed whether the result the program yields is correct or not. Display all this information in a table and add brief explanation of why some programs are not giving the correct result}

The tests have been made using the \emph{scr4\_test.sh} script. We have submitted them to the cluster, in order to make a sufficiently large number of repetitions (50) and be able to get good results by making the means. We have used 6 cores.

\begin{figure}[h]
    \input{./tables/table_e4}
    \centering
    \caption{Times (seconds) and result of all the programs}
\end{figure}

As we can see, the results of the $\pi$ approximation are the same in all the programs. (And it is the best approximation that can be expressed with the precission that these programs have by default, just 6 decimal places).

The best performance is given by versions \emph{pi\_par3}, \emph{pi\_par4} and \emph{pi\_par5}, which have a speedup (with respect to \emph{pi\_serie}) higher than 7. The next best one is \emph{pi\_par7}, with over 5.5 speedup. And the worst are \emph{pi\_par1}, \emph{pi\_par2} and \emph{pi\_par6}, with a speedup of just over 2.3.

\greyItem{Regarding pi\_par2, does it make sense to declare sum as private variable? What does it happen when you declare a pointer as private}

In pi\_par2, the variable sum (a pointer) is marked with the clause \emph{firstprivate}. As a result, each thread creates a copy of the pointer and the value its initialized with the one before the parallel region. So even though each thread has its own pointer, all of them point to the same memory position. Indeed, if the clause \emph{private} is used instead of \emph{firstprivate}, we would possibly get segmentation fault as the pointer in each thread would point to an ``unknown'' position outside the proccess memory region.

Regarding how this change affects the performance, as we expect, there is no difference between \emph{pi\_par1} and \emph{pi\_par2}; aside from \emph{pi\_par2} being slightly slower (by about 10 ms). Although we could think this higher execution time is caused because \emph{pi\_par2} has to do more operations when creating the threads, because of the allocation of this new variable for each thread, this is insignificant and the slightly higher execution time that we get may be just a coincidence.

\greyItem{What are the differences between pi\_par5, pi\_par3 and pi\_par1? Explain the concept of false sharing. 
Why does pi\_par3 obtain the linesize of the cache?}

False sharing occurs when even though different threads are wiriting in different memory addresses, all of these addresses are contained in the same cache block. Every time one thread modifies its part of the block, the block becomes invalid for the rest of the threads, and threfore they need to replace the block in their cache.

Version \emph{pi\_par1} (and \emph{pi\_par2}) is a clear example of false sharing: the k-th thread modifies the posisiton \emph{k} of the array \emph{sum} provoking that the block is no longer valid for the rest of threads. This occurs for every iteration of the loop and as result the execution time is higher than the one obtained with the serial version. Another example of false sharing occurs in version pi\_par4. However, this time it may only happen when the value of \emph{priv\_sum} is copied to the array \emph{sum}; this is once for every thread, and only just before the thread finishes, so it does not have a significant effect (if any) on execution time. On the other hand, \emph{pi\_par3} still uses an array for storing the partial sums of each thread but avoids false sharing (as discussed in the next paragraph). Finaly, version \emph{pi\_par5} uses a private variable for storing the partial sums (\emph{sum}) and after the calcultion each threads add the result to the shared variable \emph{pi}. To avoid data races, that part of the code is encapsulated with the clause \emph{critical} which ensures mutual exclusion between the threads.

The version \emph{pi\_par3} obstains the linesize of the cache to ensure that the positions of the array \emph{sum} that are modified by each thread are located in different blocks. That is to say, the postion modifies by thread 1 will be placed in a different block that the position modified by thread 2 and so on. The small drawback is that we are allocating more memory than used: in each block (64 bytes), we are only using 8 bytes. 

\greyItem{What is the effect of using the pragma critical? How is the performance? Why does this happen?}

As mentioned before, this clause ensures mutual exclusion for an specific part of the code. This clause is used in 

\end{enumerate}

\pagebreak

\section{Exercise 5: Optimization	of	calculation	programs}

\begin{enumerate}[label=5.\arabic*,leftmargin=*]
    \setcounter{enumi}{-1}

%0
\greyItem{Compile and run the program using some images as arguments. Examine the results that were generated and analyze briefly the provided program.}


%1
\greyItem{The program includes an outermost loop that iterates over the arguments applying the algorithms to each of the arguments (indicated as Loop 0). Is this loop optimal to be parallelized?}

It may make sense to think that this loop could be parallelized when the task we want to perform is processing a high number of images. This way, each thread would process a different image at a time.

However, this may not be the optimal approach. For example, if the images vary in size, some thread would have a lot more work to do than others, wasting resources that coul help finish the task. There are other drawbacks, as explained in the next questions.

    \begin{enumerate}
        \greyItem{What happens if fewer arguments are passed than number of cores?}
            
        The remaining cores will be idle, while the others work. This is a pretty inefficient way to distribute the tasks.

        \greyItem{Suppose you are processing images from a space telescope that occupy up to 6GB each, is this the right option? Comment how much memory each thread consumes based on the size in pixels of the input image.}
    \end{enumerate}

%2
\greyItem{During the previous task (task 3), we observed that the order of access to the data is important. Are there any loops that are accessing the data in a suboptimal order? Please correct it in such case.}


\begin{enumerate}
        \greyItem{It is imperative that the program continue to perform the same algorithm, so only changes should be made to the program that do not change the output.}
            
        \greyItem{Explain why the order is not correct if you change it.}
    \end{enumerate}
    
All nested loops in \emph{main()} access data in the suboptimal order. For instance, accessing: 
\begin{equation}\label{array}
    \texttt{grey\_image[j * width + i]}
\end{equation}
This is equivalent to accessing column $i$, row $j$. Therfore, the inermost loop should iterate on $i$, accessing contiguous data in each iteration of this loop. However, the loops in the provided code do exactly the opposite, which is why we have changed them. 

In each of the nested loops inside \emph{Loop 0} we can change the order, because the instructions performed inside the innermost loop are independent from one another, which means that the order in which they are performed does not alter the result. This is similar to what we did when changing the order in which the elements were added when calculating the sum of the elements of a matrix.

In the first loop (RGB to grey scale), an element of an array is accessed in each iteration (using \ref{array}). So there is no problem.

The next loop (Sobel edge detection) does something similar, writing in array \emph{edges}. It also reads from array \emph{grey\_image}, but reading does not matter when we worry about changing order of insstructions. Again, there is no alteration in the result if we change the order of this loop.

The last loop is a nested loop on indices $i$ and $j$, which contains another nested loop on indices $p_1$ and $p_2$. The loops on $i$ and $j$, apart from iterating on $p_1$ and $p_2$, only write to an array: \emph{edges\_denoised}, in different positions each time. The $p_1$ and $p_2$ loops write into an array in order. This is something to take into account, as changing order in the indices would alter the result of this array; however, this array is later sorted, so the order in which it was filled is not relevant. At last, this loop can also be optimized without changing the result of the computation.

This improvement has been implemented in file \emph{edgeDetector\_optLoops.c}, and in all of the parallel improvements.

%3
\greyItem{Bypassing Loop 0, test different parallelizations with OpenMP explaining which ones should get better performance.}
    \begin{enumerate}
        \greyItem{It is imperative that the program continue to perform the same algorithm, so only changes should be made to the program that do not change the output.}
            
        \greyItem{It is not necessary to fully explore all the possible parallels. It is necessary to use the knowledge obtained in this task to define which would be the best solutions. Explain the reasons in the
        document.}
    \end{enumerate}


%4
\greyItem{Fill in a table with time and speedup results compared to the serial version for images of different resolutions (SD, HD, FHD, UHD-4k, UHD-8k). You must include a column with the fps at which the program would process}

%5
\greyItem{Something that we have left aside is to use compiler optimizations. Repeat the previous section adding the -O3 flag to the gcc command. Obtain information about the optimizations the compiler implements (and how they might affect our parallelization). Does the compile implement loop unrolling? Is this option activated? Does the compiler implement some kind of vectorization? Note: the -O3 option can generate warnings when compiled. Check that the output remains the same in any case.}

\end{enumerate}



\end{document}


\begin{lstlisting}[language=C, texcl=true]
    // RGB to grey scale
    int r, g, b;
    for (int i = 0; i < width; i++)
    {
        for (int j = 0; j < height; j++)
        {
            getRGB(rgb_image, width, height, 4, i, j, &r, &g, &b);
            grey_image[j * width + i] = (int)(0.2989 * r + 0.5870 * g + 0.1140 * b);
        }
    }  
\end{lstlisting}





