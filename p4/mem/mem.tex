\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage[utf8]{inputenc}

\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{listings}  
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{.2,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0.4,0.82}
\definecolor{codeorange}{rgb}{0.94,0.34,0.0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backcolourgray}{rgb}{0.92,0.92,0.92}
\definecolor{codewhite}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolourgray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    extendedchars=true,
    frame=single
    %, basicstyle=\footnotesize
}
\lstset{style=mystyle}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{Arquitectura de ordenadores. Practice 4.}
\lhead{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
\cfoot{\thepage}



%\setlength{\parskip}{0.15cm}


%parameters: file, caption, label, scale
\newcommand{\myFigure}[4]{%
    \begin{figure}[!ht]
        \includegraphics[scale=#4]{#1}
        \centering
        \caption{#2}
        \label{#3}
    \end{figure}
}
%grey item for enumerate
\newcommand{\greyItem}[1]{\item\emph{\textcolor{darkgray}{#1}}}


\begin{document}


\title{\textbf{Arquitectura de ordenadores. Practice 4.}}
\author{\textbf{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}\\(group 1392, team 04)}
%\date{}
\maketitle

\begin{tcolorbox}
\tableofcontents
\end{tcolorbox}


\newpage
\section{Exercise 1: Basic OpenMP programs}

\begin{enumerate}[label=1.\arabic*]
\greyItem{Is it possible run more threads than cores on the system? Does it make sense to do it?}

It is possible to do so: if we first execute omp1 without arguments to see the available cores and then we execute it again with a number of threads higher than the amount of cores, we will see that the execution is carried out without problems. Using more threads than cores may make no sense when the goal is simply to parrallelize a single task because the threads will interrupt each other. However, if there are several tasks to be run at the same time and any of them may provoke the thread to go asleep, then using more threads could help to take advantage of the time that particular thread is asleep. 

%Quitar ultimo párrafo??

\greyItem{How many threads should you use on the computers in the lab? And in the cluster? And on your own team?} 

For a general situation, the number of threads to be used is the one determined by the function \emph{omp\_get\_max\_threads()}. Indeed this value used by default.

\greyItem{Modify the omp1.c program to use the three ways to choose the number of threads and try to guess the priority among them}

The priority order is: \texttt{}{OMP\_NUM\_THREADS} $<$ \texttt{omp\_set\_num\_threads()} $<$ \#pragma\ omp parallel num\_threads(numthr) 

\greyItem{How does OpenMP behave when we declare a private variable?}

Each thread created will create its own (new) variable.

\greyItem{What happens to the value of a private variable when the parallel region starts executing?}

The variable is not necessarily initialized when allocated in each thread stack, although when executing \texttt{omp2} in the lab computers, it is initialized as if it were a \emph{firstprivate} variable (with the value of the variable previous to the parallel section). Furthermore, it is private to each thread so its value will only change because of instructions performed by that specific thread.

\greyItem{What happens to the value of a private variable at the end of the parallel region?}

After the parallel region, the master threads resumes execution and the value of the variable is the one from before the parallel execution. Privates variable are local to the threads and the parallel region, so they are deleted when the threads terminate.

\greyItem{Does the same happen with public variables?}

Public variables are shared by all threads (no copies of the variable are created for each thread); threfore, after the parallel region, the value will be determined by the code executed in the parallel region (and, perhaps, by the order of execution of the several threads).

\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2: Parallelize the dot product}

\begin{enumerate}[label=2.\arabic*]

\greyItem{Run the serial version and understand what the result should be for different vector sizes.} 
    
As we are calculating the dot product of two vectors of size M where all the components are 1, the expected output is M.
    
\greyItem{Run the parallelized code with the openmp pragma and answer the following questions in the document}
    
The parallel version is not working due to the fact that variable sum is being shared (default mode). So we have severals threads writing in the same variable at the same time which produces a data race.

\greyItem{Modify the code and name the program pescalar\_par2. This version should give the correct result using the appropriate pragma:}

The modification needed is adding the pragma clause just before the calculation.
Both \emph{pragmas} solve the data race. Nonetheless, the way they achieved so is different. \emph{Critial} pragma implies the use of a mutex so only one thread can access the block at a time (which is quite expensive in execution time) whereas \emph{atomic} ensures that the whole calculation (line 42) is executed as a single operation. The result is exactly the same as we are enclosing just a sentence of code, however, the second option is considerably faster.

\begin{lstlisting}[language=C, texcl=true]
    ...
    #pragma omp parallel for 
    for(k=0;k<M;k++)
    {	
        #pragma omp atomic
            sum = sum + A[k]*B[k];
    } 
    ...
\end{lstlisting}

\greyItem{Modify the code and name the resulting program pescalar\_par3. This version should give the correct result using the appropriate pragma: }

If we use \emph{reduction} we get better results than the ones obtained with \emph{atomic}. This is the appropriate solution in this case, because it is the optimized version meant for this specific task.

\greyItem{Run time analysis}

We have written a script to test this (\emph{scr2\_threshold.sh}). As can be seen in the following figure (fig. \ref{threshold}), the parallel version (in the lab computers) is very inconsistent. However, the first value that meets the requirements is $T=1200000$

\myFigure{../material/outputs/out2_final/threshold.png}{Search for threshold in the lab computers}{threshold}{0.5}

\end{enumerate}


\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 3: Parallel matrix multiplication}

We have sampled times for the different programs using matrices of size 2000, which yield high enough times to make a difference between the different implementations. Here are the results:

\begin{figure}[h]
    \input{./tables/table3_n2000_1}
    \centering
    \caption{Times (seconds) for the requested tests}
\end{figure}

\begin{figure}[h]
    \input{./tables/table3_n2000_2}
    \centering
    \caption{Ratios for the requested tests}
\end{figure}

\begin{enumerate}[label=3.\arabic*]
\greyItem{ Which of the three versions performs the worst? Why? Which of the three versions performs better? Why?}

The worst performing version is the one that parallelizes the innermost loop, because it is forced to ``fork'' itself $N^2$ times, which causes the execution to be a lot slower. The best performing one, as could be expected, is the one that parallelizes the outermost loop, it has to separate into different threads only once (the \emph{multiplication\_loop2.c} one ``forks'' $N$ times).

\greyItem{Based on the results, do you think fine-grained (innermost loop) or coarse-grained (outermost loop) parallelization is preferable in other algorithms?}

Based on this result, coarse-grained parallelization is generally preferable, as it avoids slowing the execution by killing and creating threads multiple times.

\end{enumerate}

Taking the best parallel version (parallelizing the outermost loop) alongside the serial version, we have tested for $N$ between $512+P$ and $512+1024+P$ with increments of $64$. And the parallel version, we have tested for $2$, $4$ and $6$ threads. These are the results: 


\myFigure{../material/outputs/out3/test_cl0/t_fig.png}{Time in seconds.}{times_e3}{0.63}

As expected, the plots have a cubic behaviour. Comparing the different plots, as the values of $N$ are sufficiently large to take advantage of the parallelization, the higher number of threads are used, the less time it takes to compute the multiplication. The serial version is the slowest for all of these values of $N$.

\myFigure{../material/outputs/out3/test_cl0/r_fig.png}{Time ratios.}{ratios}{0.63}

As commented before, for the sizes that we have tested, the asymptotic behaviour is already present (as $N$ is big enough). The four plots (time fuctions) have, aproximately, a complexity of $\mathcal{O}(N^3)$. Which means that if we calculate the speedup for each parallel version (with different number of threads), it will stabilize around a constant in each case. This constant behaviour of the ratios is what can be seen in this plot. Again, as lower number of threads is worse than higher number of threads for big enough values of $N$, the constant in question is higher when the number of threads is higher.

\begin{enumerate}[label=3.\arabic*]
    \setcounter{enumi}{3}
\greyItem{3.3 If in the previous chart you did not obtain a behavior of the acceleration as a function of N that stabilizes or decreases with increasing the size of the matrix, continue increasing the value of N until you get a chart with this behavior and indicate for which value of N you begin to see the change in trend.}

The values that we got already stabilize the speedup; as can be seen in figure \ref{ratios}, the values stabilize: for 2 threads, around $2.62$; for 4 threads, around $3.1$; and 6 threads, $4.6$.

In order to check the values of $N$ that stabilize the speedup, we have to check lower values:

\myFigure{../material/outputs/out3/test_cl0_extended/r_fig.png}{Time ratios extending the plot with lower values.}{ratios_extended}{0.63}

As we can see, with increasing number of threads, the sabilizing value increases slightly. For 2 threads, the value is around $N=100$; for 4 threads, between $N=130$ and $N=400$, and for 6 threads, between $N=160$ and $N=450$.


\end{enumerate}


\section{Exercise 4: Example of numerical integration}

\begin{enumerate}[label=4.\arabic*]

\greyItem{How many rectangles are used in the program? Which value does h take?}

The numer of rectangles is determined by the variable \emph{n} whose value is ${10^8}$. The variable \emph{h} takes the middle value of the ${10^8}$ rectangles.

\greyItem{Analyze performance}

\greyItem{Regarding pi\_par2, does it make sense to declare sum as private variable? What does it happen when you declare a pointer as private}

In pi\_par2, the variable sum (a pointer) is marked with the clause \emph{firstprivate}. As a result, each thread creates a copy of the pointer and the value its initialized with the one before the parallel region. So even though each thread has its own pointer, all of them point to the same memory position. Indeed, if the clause \emph{private} is used instead of \emph{firstprivate}, we would get segmentation fault as the pointer in each thread would point to an ''unknown'' position outside the proccess memory region.

\greyItem{What are the differences between pi\_par5, pi\_par3 and pi\_par1? Explain the concept of false sharing. 
Why does pi\_par3 obtain the linesize of the cache?}

False sharing occurs when the same block of memory is in the caches of different threads and it is being modified by more than one of them. Every time one thread modifies part of the block, the block becomes no valid for the rest of the threads, and threfore they need to replace the block in the cache.

Version pi\_par1 (and pi\_par2) is a clear example of false sharing: the k-th thread modifies the posisiton \emph{k} of the array \emph{sum} provoking that the block is no longer valid for the rest of threads. This occurs for every iteration of the loop and as result the execution time is largen than the one obtained with the serial version. Another example of false sharing occurs in version pi\_par4. However, this time it may only happen when the value of \emph{priv\_sum} is copied to the array \emph{sum}. On the other hand, pi\_par3 still uses an array for storing the partial sums of each thread but avoids false sharing (discussed in the next paragraph). Finaly, version pi\_par5 uses a private variable for storing the partial sums (\emph{sum}) and after the calcultion each threads add the result to the shared variable \emph{pi}. To avoid data races, that part of the code is encapsulated with the clause \emph{critical} which ensures mutual exclusion between the threads.

The version pi\_par3 obstains the linesize of the cache to ensure that the positions of the array \emph{sum} that are modifies by each thread are located in different blocks. That is to say, the postion modifies by thread 1 will be placed in a different block that the position modified by thread 2 and so on. The small drawback is that we are allocating more memory than used: in each block (64 bytes), we are only using 8 bytes. 

\greyItem{What is the effect of using the pragma critical? How is the performance? Why does this happen?}

As mentioned before, this clause ensures mutual exclusion for an specific part of the code. This clause is used in 

\end{enumerate}


\end{document}


\begin{lstlisting}[language=C, texcl=true]
    typedef struct Mine_struct_{
        long int target;
        long int begin; //índice por el que empieza a buscar el hilo
        long int end; //índice hasta el que el hilo busca (no incluido)
    } Mine_struct;  
\end{lstlisting}





