\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage[utf8]{inputenc}


\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancyvrb}

\usepackage{tcolorbox}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{.2,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0.4,0.82}
\definecolor{codeorange}{rgb}{0.94,0.34,0.0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backcolourgray}{rgb}{0.92,0.92,0.92}
\definecolor{codewhite}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolourgray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    extendedchars=true,
    frame=single
    %, basicstyle=\footnotesize
}
\lstset{style=mystyle}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{Arquitectura de ordenadores. Practice 3.}
\lhead{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
\cfoot{\thepage}



%\setlength{\parskip}{0.15cm}


%parameters: file, caption, label, scale
\newcommand{\myFigure}[4]{%
    \begin{figure}[!ht]
        \includegraphics[scale=#4]{#1}
        \centering
        \caption{#2}
        \label{#3}
    \end{figure}
}


\begin{document}


\title{Arquitectura de ordenadores. Practice 3.}
\author{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
%\date{}
\maketitle

%\begin{tcolorbox}
%\tableofcontents
%\end{tcolorbox}

\addcontentsline{toc}{subsection}{Exercise 0}
\subsection*{Exercise 0}

We have been submitting our tasks to the \emph{MV} queue for all exercises of the practice because it returned the best (smoothest) results. 

After running the commands shown in the statement, we got these results. 

As can be seen in figure \ref{cache_conf}, the first level is the only one where there are two different caches: one for data and another for instructions. Both are of the same kind: size of 32768 B (32 KB), with blocks of 64 B and 8 ways. 
In the second level, there is a unique cache which is larger (2097152 B = 2 MB) but has the same degree of associativity. For the third level, the size increases (16777216 B = 16 MB) as well as the associativity (16 ways) whereas the block size remains the same.

\begin{figure}[h]
    \lstinputlisting[]{../material_P3/out0/mv/cache.dat}
    \centering
    \caption{Output of \texttt{getconf -a | grep -i cache}}
    \label{cache_conf}
\end{figure}

\pagebreak

Here is the info of the each of the processors (this is processor 0, but they are all the same):

\begin{figure}[h]
    \lstinputlisting[firstline=0,lastline=18]{../material_P3/out0/mv/cpuinfo.dat}
    \centering
    \caption{First lines of \texttt{/proc/cpuinfo}}
    \label{cpuinfo}
\end{figure}

The following diagram shows the archiquetecture of the different caches in each of the 8 cores. 

\myFigure{../material_P3/out0/mv/figure.png}{Topology of the used CPU.}{topology}{0.34}

\pagebreak

\addcontentsline{toc}{subsection}{Exercise 1}
\subsection*{Exercise 1}

The following image shows a graph with the execution time of both programs for different sizes of the matrix.

\myFigure{../material_P3/out1/mv_att4/slow_fast_time.png}{Slow vs Fast execution times.}{slow_fast_times}{0.45}

As pointed by the graph the execution time for small values of N is similar for both programs (most of the matrix can be stored completely in the cache). However as N grows the execution time for the slow program shows a cuadratic behaviour while the fast version barely grows.

This is due to the fact that the matrix is stored by rows in main memory, and therefore stored in the cache by rows too. The slow version access the matrix by columns which leads to a higher number of misses and consequently, a higer number of accesses to main memory. 

For obtaining the results, it is necessary to run the simulation several times in order to calculate a mean and have more than one result, so that the programs do not present abnormal times because of other factors different from what we are measuring. Furthermore, the chosen execution pattern alternates between the slow and fast version to prevent the programs from reusing the cache from the previous execution (with the same size).


\addcontentsline{toc}{subsection}{Exercise 2}
\subsection*{Exercise 2}

First, we will analyze the write misses: 

\myFigure{../material_P3/out2/mv_att1/cache_escritura.png}{Cache write misses.}{cache_escritura}{0.4}

It may seem that only four lines have been plotted, however this is not the case. (We have plotted the slow graphs with a bit of transparency so that the fast graphs can also be seen). The slow and fast version have (almost) the same number of write misses (for the same execution values of N and size of the cache). The reason behind this is that both programs fill (write) the initial matrix in the same way, by rows (\texttt{generateMatrix(int size)}).

On the other hand, the read misses show different results:

\myFigure{../material_P3/out2/mv_att1/cache_lectura.png}{Cache read misses.}{cache_lectura}{0.4}

This time, we get 8 different lines, as one could expected beforehand. The dotted lines shows the fast version that necessarily must have less read misses than the fast one (when compared with the same cache size). Again this is due to the fact that the matrix is stored by rows (one row beside the next, sequentially), and the slow version accesses by columns, while the fast version accesses by rows.

The effect of the cache size is also observed as the fast version for the smallest cache produces more misses than the slow version of the two largest caches. In general, it is to be expected that smaller sized caches would provoke a higher number of misses for the same program; which is what we see in figure \ref{cache_lectura}.




\addcontentsline{toc}{subsection}{Exercise 3}
\subsection*{Exercise 3}

\myFigure{../material_P3/out3/mv_att2/mult_time.png}{Multiplication execution time}{mult_time}{0.4}

The image above (figure \ref{mult_time}) shows the execution time for the two different multiplication methods. The transposed multiplication is significantly faster as it accesses matrixes by rows when multiplying. On the other hand, in the regular multiplication, the second operand matrix is accessed by columns which produces a high rate of read misses and as a result a higher execution time.

\pagebreak

When simulating with \emph{valgrind}, we get very similar results to what we saw in exercise 2 (figure \ref{cache_lectura}). The less optimized program (regular multiplication), which reads matrix $B$ by columns (instead of by rows), produces a higher number of read misses than the ``transposed multiplication'', which accesses matrix $B^t$ by rows.

COMENTAR LA ESCALONACIÓN(?)

\myFigure{../material_P3/out3/mv_att2/mult_cache.png}{Cache misses of the multiplication programs}{mult_misses}{0.45}

The write misses cannot be appreciated in figure \ref{mult_misses}, as they are much lower in value than the read misses, so we have plotted them in a separate graph (figure \ref{mult_misses_write}). Like in exercise 2, both programs write in the same manner: when generating matrixes $A$ and $B$, and when writing the result in matrix $C=AB$. So, as expected, the write misses are very similar. If we look closely at the exact numbers, we can see that the number of write misses is a bit higher (by less than $10$ units) in the transposed version (for all sizes of the matrixes). This is completely normal, bacause the transposed multiplication has to (re)write most of the values of matrix $B$ when transposing it before multiplying; this is what causes the (insignificant) extra write misses in this program, which do not appear in the other one. 

\myFigure{../material_P3/out3/mv_att2/mult_cache_write.png}{Cache write misses of the multiplication programs}{mult_misses_write}{0.4}

\end{document}


\begin{lstlisting}[language=C, texcl=true]
    typedef struct Mine_struct_{
        long int target;
        long int begin; //índice por el que empieza a buscar el hilo
        long int end; //índice hasta el que el hilo busca (no incluido)
    } Mine_struct;  
\end{lstlisting}





