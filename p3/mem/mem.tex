\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage[utf8]{inputenc}

\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{float}
\usepackage{listings}  
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{.2,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0.4,0.82}
\definecolor{codeorange}{rgb}{0.94,0.34,0.0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{backcolourgray}{rgb}{0.92,0.92,0.92}
\definecolor{codewhite}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolourgray},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{codeblue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    extendedchars=true,
    frame=single
    %, basicstyle=\footnotesize
}
\lstset{style=mystyle}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\pagestyle{fancy}
\fancyhf{}
\rhead{Arquitectura de ordenadores. Practice 3.}
\lhead{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
\cfoot{\thepage}



%\setlength{\parskip}{0.15cm}


%parameters: file, caption, label, scale
\newcommand{\myFigure}[4]{%
    \begin{figure}[!ht]
        \includegraphics[scale=#4]{#1}
        \centering
        \caption{#2}
        \label{#3}
    \end{figure}
}


\begin{document}


\title{Arquitectura de ordenadores. Practice 3.}
\author{Pablo Cuesta Sierra, Álvaro Zamanillo Sáez}
%\date{}
\maketitle

%\begin{tcolorbox}
%\tableofcontents
%\end{tcolorbox}

Note: during this whole document (in the plots), when we write ``Matrix size ($N$)'', it means we are plotting (normally in the $x$-axis) the values of $N$, where $N\times N$ is the size of the used matrices.

\addcontentsline{toc}{subsection}{Exercise 0}
\subsection*{Exercise 0}

We have been submitting our tasks to the \emph{MV} queue for all exercises of the practice because it returned the best (smoothest) results. 

After running the commands shown in the statement, we got these results. 

As can be seen in figure \ref{cache_conf}, the first level is the only one where there are two different caches: one for data and another for instructions. Both are of the same kind: size of 32768 B (32 KB), with blocks of 64 B and 8 ways. 
In the second level, there is a unique cache which is larger (2097152 B = 2 MB) but has the same degree of associativity. For the third level, the size increases (16777216 B = 16 MB) as well as the associativity (16 ways) whereas the block size remains the same.

\begin{figure}[h]
    \lstinputlisting[]{../material_P3/out0/mv/cache.dat}
    \centering
    \caption{Output of \texttt{getconf -a | grep -i cache}}
    \label{cache_conf}
\end{figure}

\pagebreak

Here is the info of each of the processors (this is processor 0, but they are all the same):

\begin{figure}[h]
    \lstinputlisting[firstline=0,lastline=18]{../material_P3/out0/mv/cpuinfo.dat}
    \centering
    \caption{First lines of \texttt{/proc/cpuinfo}}
    \label{cpuinfo}
\end{figure}

The following diagram shows the archiquetecture of the different caches in each of the 8 cores. 

\myFigure{../material_P3/out0/mv/figure.png}{Topology of the used CPU.}{topology}{0.34}

\pagebreak

\addcontentsline{toc}{subsection}{Exercise 1}
\subsection*{Exercise 1}

The following image shows a graph with the execution time of both programs for different sizes of the matrix.

\myFigure{../material_P3/out1/mv_att4/slow_fast_time.png}{Slow vs Fast execution times.}{slow_fast_times}{0.65}

As pointed by the graph, the execution time for small values of N is similar for both programs (most of the matrix can be stored completely in the cache). However, as N grows, the execution time for the slow program shows a significant cuadratic behaviour while the fast version barely grows.

This is due to the fact that the matrix is stored by rows in main memory, and therefore, it is stored in the cache by rows too. The slow version access the matrix by columns which leads to a higher number of misses and consequently, a higer number of accesses to main memory (hundreds of times slower). 

For obtaining the results, it is necessary to run the simulation several times in order to calculate a mean and have more than one result, so that the programs do not present abnormal outliers because of other factors different from what we are measuring. Furthermore, the chosen execution pattern alternates between the slow and fast version to prevent the programs from reusing the cache from the previous execution (with the same size).

\pagebreak

\addcontentsline{toc}{subsection}{Exercise 2}
\subsection*{Exercise 2}

First, we will analyze the write misses: 

\myFigure{../material_P3/out2/mv_att1/cache_escritura.png}{Cache write misses.}{cache_escritura}{0.55}


It may seem that only four lines have been plotted, however this is not the case. (We have plotted the slow graphs with a bit of transparency so that the fast graphs can also be seen). The slow and fast version have (almost) the same number of write misses (for the same execution values of N and size of the cache). The reason behind this is that both programs fill (write) the initial matrix in the same way, by rows, using the function \texttt,{generateMatrix(int size)}.

On the other hand, the read misses show different results:

\myFigure{../material_P3/out2/mv_att1/cache_lectura.png}{Cache read misses.}{cache_lectura}{0.55}

This time, we get 8 different lines, as one could expected beforehand. The dotted lines show the fast version, which necessarily must have less read misses than the fast one (when compared with the same cache size). Again, this is due to the fact that the matrix is stored by rows (one row beside the next, sequentially), and the slow version accesses the matrix by columns, while the fast version accesses it by rows.

The effect of the cache size can also be observed as the fast version for the smallest cache produces more misses than the slow version of the two largest caches. In general, it is to be expected that smaller sized caches would provoke a higher number of misses for the same program; which is what we see in figure \ref{cache_lectura}.



\addcontentsline{toc}{subsection}{Exercise 3}
\subsection*{Exercise 3}

The image below (figure \ref{mult_time}) shows the execution time for the two different multiplication methods. The transposed multiplication is significantly faster as it accesses matrixes by rows when multiplying. On the other hand, in the regular multiplication, the second operand matrix is accessed by columns which produces a high rate of read misses and as a result a higher execution time.

Commenting on the strange peaks that appear in figure \ref{mult_time}, we do not know the real reason why this happens. These peaks are neither because of some mistake on calculating values, nor inconsistencies in execution that could be solved repeating the tests. The values of over 10 seconds when executing for size $N=1408$ are pretty consistant when executing in queue \emph{mv.q} of the cluster (the same goes for the rest of the matrix sizes); meaning that the time in each iteration does not vary more than $0.1s$ for each $N$. If we execute the same script in another queue (\emph{amd.q}, for instance), peaks still appear, but not necessarily in the same sizes ($N$). We think this strange and counterintuitive beahviour might be caused by inefficient use of caches, and that certain sizes of matrixes are managed more efficiently by the cache (because its size is a multiple of the block size, for example) than others; even if this other ones are smaller.

\myFigure{../material_P3/out3/mv_att2/mult_time.png}{Multiplication execution time}{mult_time}{0.65}

When simulating with \emph{valgrind} (figure \ref{mult_misses}), we get very similar results to what we saw in exercise 2 (figure \ref{cache_lectura}). The less optimized program (regular multiplication), which reads matrix $\mathbf{B}$ by columns (instead of by rows), produces a higher number of read misses than the ``transposed multiplication'', which accesses matrix $\mathbf{B}^T$ by rows.

The regular matrix multiplication shows a step pattern every two times we increase the value of N. This may be due to alignment of the data as we are increasing the matrix by 32 and the block size of the cache is 64B. 

\myFigure{../material_P3/out3/mv_att2/mult_cache.png}{Cache misses of the multiplication programs}{mult_misses}{0.6}

The write misses cannot be appreciated in figure \ref{mult_misses}, as they are much lower in value than the read misses, so we have plotted them in a separate graph (figure \ref{mult_misses_write}). Like in exercise 2, both programs write in the same manner: when generating matrixes $\mathbf{A}$ and $\mathbf{B}$, and when writing the result in matrix $\mathbf{C}=\mathbf{AB}$. So, as expected, the write misses are very similar. If we look closely at the exact numbers, we can see that the number of write misses is a bit higher (by less than $10$ units) in the transposed version (for all sizes of the matrixes). This is completely normal, bacause the transposed multiplication has to (re)write most of the values of matrix $\mathbf{B}$ when transposing it before multiplying; this is what causes the (insignificant) extra write misses in this program, which do not appear in the other one. 

\myFigure{../material_P3/out3/mv_att2/mult_cache_write.png}{Cache write misses of the multiplication programs}{mult_misses_write}{0.4}


\addcontentsline{toc}{subsection}{Exercise 4}
\subsection*{Exercise 4}

For this exercise, we have distinguished two main experiments. In each of them, we modify a different parameter of the cache, leaving the rest untouched. For both, as requested, we use the programs written for exercise 3: \emph{multiplication.c} and \emph{multiplication\_t.c}


\addcontentsline{toc}{subsubsection}{Experiment 1: associativity}
\subsubsection*{Exercise 4. Experiment 1: associativity}

Here we simulate (using \emph{valgrind}) the execution of both programs. We fix the line (block) size (64B), as well as the cache sizes: level 1 caches (both data and instructions) are of size 4kB, and level 2 cache is of size 8MB. For each experiment, we test with matrix sizes ($N\times N$) for $N\in\{3072,3328,3584,3840,4096\}$. The difference between experiments is the type of set associative caches. We test for 1-way, 2-way, 4-way and 8-way set associative caches.

These are the results:

\myFigure{../material_P3/protect_out4/asoc/p2/4096/lectura.png}{Cache read misses of the multiplication programs (varying associativity)}{asoc_lectura}{0.55}

\myFigure{../material_P3/protect_out4/asoc/p2/4096/escritura.png}{Cache write misses of the multiplication programs (varying associativity)}{asoc_escritura}{0.55}

First of all, we will comment on the difference between both programs. Write misses (figure \ref{asoc_escritura}), for a fixed type of cache, are the same in both programs, as we already commented in exercise 3 (for figure \ref{mult_misses_write}). Read misses, on the other hand, are different. As could be expected, regular multiplication generates a much higher number of write misses than ``transposed multiplication''. Again, this is due to the fact that transposed multiplication accesses both matrices by rows, which is how they are stored in memory; making it so that data accessed is always (when multiplying) next to the previous data which has been recently used; whereas regular multiplication traverses matrix $\mathbf{B}$ by columns, making ``jumps'' in memory, and making recently loaded blocks of the cache useless.

Next, let us take a look at the differences caused by the changes in associativity. There are three distinguished sections in our plots (already explained in last paragraph): cache write misses, cache read misses of regular multiplication, and cache read misses of transposed multiplication. In all of these sections, the 1-way cache generates the highest number of misses, followed by the 2-way cache; 4-way and 8-way cache yield almost the exact same numbers, which are very close below the 2-way cache. 

These results do actually make sense, and are consistant with the performance explained in the theory slides. The line size of the caches is 64B, and there are $\frac{4096}{64}=64$ blocks in the (level 1) cache. 
Furthermore, each cache block contains 8 elements of a matrix (which means that, at a time, there cannot be more than 512 elements of a matrix in cache). Therefore, it is generally the case that a row fills several blocks.
This means that when $\mathbf{A}$ is being multiplied by $\mathbf{B}$, it frequently occurs (more than once per element $c_{i,j}$ of the result $\mathbf{C}=\mathbf{AB}$) that the current block being used of $\mathbf{A}$ has the same index as the one being used from $\mathbf{B}$. This implies that, if the cache is 1-way associative, the two current blocks will be replacing each other (which is known as conflict misses); this causes the much higher number of misses for this kind of cache. However, when associativity increases, we not longer have the problem that the two current blocks rewrite each other, so the number of misses decreases significantly.


\addcontentsline{toc}{subsubsection}{Experiment 2: block size}
\subsubsection*{Exercise 4. Experiment 2: block size}


For this second experiment, we are going to execute the two multiplication methods with caches of different sizes (2048B - 16384B) and see how the block size affects each time to the resulting misses. (The caches used in the simulations are all 1-way associative.)

First of all, we are going to consider only read misses:


\myFigure{../material_P3/protect_out4/BlockSize_2048/cache_lectura.png}{Cache read misses with a cache of 2048B}{block_2048_lec}{0.52}

\pagebreak

\myFigure{../material_P3/protect_out4/BlockSize_4096/cache_lectura.png}{Cache read misses with a cache of 4096B}{block_4096_lec}{0.52}


\myFigure{../material_P3/protect_out4/BlockSize_8192/cache_lectura.png}{Cache read misses with a cache of 8192B}{block_8192B_lec}{0.52}

\pagebreak

\myFigure{../material_P3/protect_out4/BlockSize_16384/cache_lectura.png}{Cache read misses with a cache of 16384B}{block_16384B_lec}{0.52}


The first thing we notice is that as we increase the size of the cache, the lines for each block size configuration get closer (take for instace the case of a 16384B cache -- figure \ref{block_16384B_lec}).  Therefore, it makes sense to think that the impact of the block size will be greater when the size of the elements of the matrix is of the order of the cache and for small caches.

The key aspect is going to be the number of lines of the cache for each configuration. In order to determine this number, we have to do the following calculation: 
\[
\text{Number of lines} = \frac{\text{Cache Size}}{\text{Block Size}}
\]

Let's focus now in the case of a 2048B cache. If we subsitute the different values of the block size, we get these numbers of lines:

{
\begin{table}[!ht]
    \centering
    \begin{tabular}{lllll}
    \hline
    \multicolumn{1}{|l|}{Block Size}      & \multicolumn{1}{l|}{32} & \multicolumn{1}{l|}{64} & \multicolumn{1}{l|}{128} & \multicolumn{1}{l|}{256} \\ \hline
    \multicolumn{1}{|l|}{Number of lines} & \multicolumn{1}{l|}{64} & \multicolumn{1}{l|}{32} & \multicolumn{1}{l|}{16}  & \multicolumn{1}{l|}{8}   \\ \hline                    
    \end{tabular}
    \caption{\label{tab:table-name}Relation between block size and number of lines for a 1-way 2048B cache.}
\end{table}
}


When the block is 32 bytes long, we can store 4 elements of the matrix in each block (the type used in the matrix has a size of 8 bytes). When obtaining the element $c_{1,1}$ we have to fetch the first part of row 1 of matrix $\mathbf{A}$ and the first part of rows 1-4 of matrix $\mathbf{B}$ (we have to fecth rows 1-4 because we have only the first 4 elements of row 1 of matrix $\mathbf{A}$). As we have 64 rows, we have no need to replace. However, let see what happens when the block is 256 bytes, we have only 6 lines in the cache. Again for calculating element $c_{1,1}$ we first fecth the first part of row 1 of matrix A; this time this first part of the row contains 32 elements. Then we start fetching the beginning of the rows 1-32 of matrix B. As we only have 8 lines, we will have to replace blocks. 

So the problem in using big block sizes is that we limit the number of rows in the cache. Furthermore, it makes no sense having large blocks when doing the regular multiplication as we only need the first elements of each block. In other words, we are fetching huge blocks just to use a few bytes of them.

We could do the same analysis for the case of a 16384B cache. 
This time the number of lines when the block is 256B is 64, which is bigger than 32 (the number of elements that can be stored in a block).
Thus, we are not getting the problem discussed above and the result does not differ much to the ones obtained with smaller values of the block size.

\pagebreak

Now, we will look at wrie misses:
Once again, we see that the number of write misses is the same for the regular and transpose multiplication as expected. The effect of the block size is similar to the one described for the read misses. The main idea is that when the cache is small and we use a large block, we reduce significantly the number of lines. Therefore, when multiplying both matrix and writing the result, there are going to be many replaces between the blocks used for calculation and the block used for writing the result. Just to ilustrate this conflict, let's recall the case of a 2048B cache with 256B blocks. In this situation, the cache has 8 lines whose content alternates between part of the rows of the operands (which produces read misses) and part of the row of the result matrix (write misses). The scarcity of lines is what provokes a high rate of misses. Nonetheless, as we increase the size of the cache, the number of lines is "big" enough even if we use a block size of 256B (figure \ref{block_8192B_esc}). 


\myFigure{../material_P3/protect_out4/BlockSize_2048/cache_escritura.png}{Cache write misses with a cache of 2048B}{block_2048_esc}{0.52}


\myFigure{../material_P3/protect_out4/BlockSize_4096/cache_escritura.png}{Cache write misses with a cache of 4096}{block_4096_esc}{0.52}

\myFigure{../material_P3/protect_out4/BlockSize_8192/cache_escritura.png}{Cache write misses with a cache of 8192B}{block_8192B_esc}{0.52}

%\myFigure{../material_P3/protect_out4/BlockSize_16384/cache_escritura.png}{Cache write misses with a cache of 16384B)}{block_16384B_esc}{0.55}



\end{document}


\begin{lstlisting}[language=C, texcl=true]
    typedef struct Mine_struct_{
        long int target;
        long int begin; //índice por el que empieza a buscar el hilo
        long int end; //índice hasta el que el hilo busca (no incluido)
    } Mine_struct;  
\end{lstlisting}





